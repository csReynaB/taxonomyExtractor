{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T15:33:34.537172Z",
     "start_time": "2025-07-04T15:33:34.503152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from networkx.algorithms.minors.contraction import identified_nodes\n",
    "\n",
    "\"\"\"\n",
    "Author: Carlos S Reyna-Blanco\n",
    "Email: carlos.reynablanco@meduniwien.ac.at\n",
    "Date: 2025-06-26\n",
    "Description: Script to retrieve organism taxonomy\n",
    "Dependencies: requests, xml.etree.ElementTree, pandas, tqdm\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from urllib.parse import quote\n",
    "import csv\n"
   ],
   "id": "fe2899c53882ca65",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T14:28:12.089626Z",
     "start_time": "2025-07-05T14:28:12.054030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_organism_from_uniprot(rep_id):\n",
    "    url = f\"https://www.uniprot.org/uniprot/{rep_id}.xml\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            #print(f\"Failed to retrieve {rep_id}: HTTP {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "        root = ET.fromstring(response.content)\n",
    "        ns = {'up': 'http://uniprot.org/uniprot'}\n",
    "\n",
    "        result = {'common': None, 'scientific': None, 'tax_id': None}\n",
    "\n",
    "        for name in root.findall('.//up:organism/up:name', namespaces=ns):\n",
    "            name_type = name.attrib.get('type', '').lower()\n",
    "            if name_type == 'common':\n",
    "                result['common'] = name.text\n",
    "            elif name_type == 'scientific':\n",
    "                result['scientific'] = name.text\n",
    "\n",
    "        taxon = root.find('.//up:organism/up:dbReference[@type=\"NCBI Taxonomy\"]', namespaces=ns)\n",
    "        if taxon is not None:\n",
    "            result['tax_id'] = taxon.attrib['id']\n",
    "\n",
    "        return result if result['scientific'] else None\n",
    "    except Exception as e:\n",
    "        #print(f\"Error processing {rep_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def lookup_organism(rep):\n",
    "    \"\"\"\n",
    "    Returns a dictionary with 'common', 'scientific', and 'tax_id' for the given rep ID.\n",
    "    First tries UniProt API.\n",
    "    If the rep ID contains underscores, tries splitting to find alternative candidates.\n",
    "    \"\"\"\n",
    "    if not rep  or not isinstance(rep, str):\n",
    "        return {'common': None, 'scientific': None, 'tax_id': None}\n",
    "\n",
    "    # Try full rep ID\n",
    "    result = get_organism_from_uniprot(rep)\n",
    "    if result:\n",
    "        return result\n",
    "\n",
    "    # Decide on splitting logic\n",
    "    if \"_\" in rep:\n",
    "        parts = rep.split(\"_\")\n",
    "    else:\n",
    "        parts = rep.split(\", \")\n",
    "\n",
    "    # Try parts of the rep ID (e.g., split by '_' or ', ')\n",
    "    for candidate in map(str.strip, parts):#rep.split(\"_\"):\n",
    "        if not candidate:\n",
    "            continue\n",
    "        result = get_organism_from_uniprot(candidate)\n",
    "        if result:\n",
    "            return result\n",
    "    #     elif candidate in manual_mapping:\n",
    "    #         return {'common': None, 'scientific': manual_mapping[candidate], 'tax_id': None}\n",
    "    #\n",
    "    # # Fallback to manual mapping using full rep\n",
    "    # if rep in manual_mapping:\n",
    "    #     return {'common': None, 'scientific': manual_mapping[rep], 'tax_id': None}\n",
    "\n",
    "    return {'common': None, 'scientific': None, 'tax_id': None}\n",
    "\n",
    "# def search_taxid_by_name(scientific_name):\n",
    "#     \"\"\"\n",
    "#     Query NCBI E-utilities for the given scientific name,\n",
    "#     return the first matching tax_id (or None).\n",
    "#     \"\"\"\n",
    "#     term = quote(str(scientific_name))  # Ensure it's a string\n",
    "#     url  = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=taxonomy&retmode=json&term={term}[SCIN]\"\n",
    "#     try:\n",
    "#         r = requests.get(url, timeout=5)\n",
    "#         r.raise_for_status()\n",
    "#         data = r.json()\n",
    "#         ids  = data.get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "#         return ids[0] if ids else None\n",
    "#     except Exception:\n",
    "#         return None\n",
    "def search_taxid_by_name(scientific_name):\n",
    "    \"\"\"\n",
    "    Query NCBI for scientific_name first.\n",
    "    If no hit, query UniProt taxonomy API and:\n",
    "      - Try the primary scientificName field\n",
    "      - Then try any synonyms or otherNames found in the UniProt entry\n",
    "    Returns the first NCBI taxid found, or None.\n",
    "    \"\"\"\n",
    "    def ncbi_lookup(name):\n",
    "        term = quote(str(name))\n",
    "        url = (\n",
    "            \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "            f\"esearch.fcgi?db=taxonomy&retmode=json&term={term}[SCIN]\"\n",
    "        )\n",
    "        try:\n",
    "            r = requests.get(url, timeout=5)\n",
    "            r.raise_for_status()\n",
    "            ids = r.json().get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "            return ids[0] if ids else None\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # 1) Try NCBI on the given name\n",
    "    taxid = ncbi_lookup(scientific_name)\n",
    "    if taxid:\n",
    "        return taxid\n",
    "\n",
    "    # 2) Fallback to UniProt taxonomy search\n",
    "    up_url = (\n",
    "        \"https://rest.uniprot.org/taxonomy/search\"\n",
    "        f\"?query={quote(str(scientific_name))}&format=json\"\n",
    "    )\n",
    "    try:\n",
    "        r = requests.get(up_url, timeout=5)\n",
    "        r.raise_for_status()\n",
    "        results = r.json().get(\"results\", [])\n",
    "        if not results:\n",
    "            return None\n",
    "\n",
    "        entry = results[0]\n",
    "        # Try the primary scientificName from UniProt\n",
    "        main_name = entry.get(\"scientificName\")\n",
    "        taxid = ncbi_lookup(main_name)\n",
    "        if taxid:\n",
    "            return taxid\n",
    "\n",
    "        # Then try any synonyms or other names\n",
    "        synonyms = entry.get(\"synonyms\", []) + entry.get(\"otherNames\", [])\n",
    "        for syn in synonyms:\n",
    "            taxid = ncbi_lookup(syn)\n",
    "            if taxid:\n",
    "                return taxid\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_lineage_from_taxid(tax_id):\n",
    "    url = f\"https://rest.uniprot.org/taxonomy/{tax_id}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            #print(f\"Failed to retrieve taxonomy for {tax_id}\")\n",
    "            return {}\n",
    "\n",
    "        data = response.json()\n",
    "        lineage = {rank['rank'].lower(): rank['scientificName'] for rank in data.get('lineage', [])}\n",
    "        lineage[data.get('rank').lower()] = data.get('scientificName')  # Add most specific name\n",
    "\n",
    "        common_name = data.get('commonName', '')  # Add common name if available\n",
    "        if not common_name:\n",
    "            # Check for 'no rank' in lineage to use as common fallback\n",
    "            common_name = lineage.get('no rank', '')\n",
    "        lineage['common'] = common_name\n",
    "\n",
    "        return lineage\n",
    "    except Exception as e:\n",
    "        #print(f\"Error retrieving lineage: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def extract_rep_id(description):\n",
    "    \"\"\"\n",
    "    Extracts the repID from a description string.\n",
    "    The function looks for a pattern like 'RepID=<rep_value>'.\n",
    "    If no repID is found, returns None.\n",
    "    \"\"\"\n",
    "    if not description:\n",
    "        return None\n",
    "\n",
    "    # Regular expression pattern: looks for \"RepID=\" followed by one or more alphanumeric or underscore characters.\n",
    "    # Adjust the pattern if your repIDs can include other characters.\n",
    "    match = re.search(r'RepID=([\\w\\-:]+)', description)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_iedb_organism_name(name):\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "\n",
    "    # Step 1: Split by '&' and take the first group\n",
    "    first_block = name.split('&')[0].strip()\n",
    "\n",
    "    # Step 2: Extract content inside the first set of square brackets\n",
    "    match = re.search(r\"\\[([^\\]]+)\\]\", first_block)\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    content = match.group(1)\n",
    "\n",
    "    # Step 3: Split by ';' and take the first entry\n",
    "    content = content.split(';')[0].strip()\n",
    "\n",
    "    # Step 4: Remove anything in parentheses\n",
    "    content = re.sub(r\"\\s*\\(.*?\\)\", \"\", content)\n",
    "\n",
    "    # Step 5: Final strip\n",
    "    return content.strip(\"'\\\" \")\n",
    "\n",
    "# Apply to each row with fallback to phage_name\n",
    "def get_organism_complete_name(row):\n",
    "    primary = clean_iedb_organism_name(row['IEDB_organism_name'])\n",
    "    if primary:\n",
    "        return primary\n",
    "\n",
    "    # Fallback to phage_name if valid\n",
    "    phage_val = row.get('phage_name', None)\n",
    "    if pd.notna(phage_val) and str(phage_val).strip().lower() not in ['false', '', 'nan']:\n",
    "        return str(phage_val).strip()\n",
    "\n",
    "    return None\n",
    "\n",
    "def collect_valid_ids(row):\n",
    "    rep_cols = ['allergome_uniprot', 'allergen_uniprot', 'iedb_uniprot', 'fummy_uniprot', 'gened_uniprot']\n",
    "    valid_ids = []\n",
    "    for col in rep_cols:\n",
    "        val = row[col]\n",
    "        if isinstance(val, str):\n",
    "            cleaned = val.strip()\n",
    "            if cleaned.lower() not in ['false', 'nan', '']:\n",
    "                valid_ids.append(cleaned)\n",
    "        elif pd.notna(val) and val is not False:\n",
    "            valid_ids.append(str(val).strip())\n",
    "    return \", \".join(valid_ids) if valid_ids else None\n",
    "\n",
    "\n",
    "def get_first_peptide_id_valid(row):\n",
    "    for val in row:\n",
    "        if isinstance(val, str) and val.strip().lower() not in ['', 'false', 'nan']:\n",
    "            return val.strip()\n",
    "        elif pd.notna(val) and val is not False:\n",
    "            return val\n",
    "    return None\n",
    "\n",
    "def extract_taxid(desc):\n",
    "    match = re.search(r'TaxID=(\\d+)', str(desc))\n",
    "    return match.group(1) if match else None"
   ],
   "id": "a31a20797a715ff7",
   "outputs": [],
   "execution_count": 320
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T17:09:27.953292Z",
     "start_time": "2025-07-04T17:09:27.945532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def match_taxid_by_approx_name(name):\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    for org_label, taxid in manual_taxids_agilent.items():\n",
    "        if org_label.lower() in name.lower():\n",
    "            return str(taxid)\n",
    "    return None\n",
    "\n",
    "def clean_prot_name_from_vfg(prot_name):\n",
    "    \"\"\"\n",
    "    If prot_name starts with 'VFG' and contains a (gb|...) section, extract the protein ID.\n",
    "    Example: \"VFG002196(gb|NP_816637) ...\" → \"NP_816637\"\n",
    "    \"\"\"\n",
    "    if isinstance(prot_name, str) and prot_name.startswith(\"VFG\"):\n",
    "        match = re.search(r'\\(gb\\|([^\\)]+)\\)', prot_name)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return prot_name\n",
    "\n",
    "def get_taxid_from_protein_id(protein_id):\n",
    "    \"\"\"\n",
    "    Given a protein accession (e.g. YP_009321702.1), fetch its TaxID and organism name from NCBI.\n",
    "    \"\"\"\n",
    "    if  not isinstance(protein_id, str) or \" \" in protein_id:\n",
    "        return None\n",
    "\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"protein\",\n",
    "        \"id\": protein_id,\n",
    "        \"retmode\": \"json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        uid = next(iter(data[\"result\"][\"uids\"]))\n",
    "        result = data[\"result\"][uid]\n",
    "        taxid = result.get(\"taxid\")\n",
    "        #organism = result.get(\"organism\")\n",
    "        return str(taxid) #{\"tax_id\": str(taxid), \"organism\": organism}\n",
    "    except Exception as e:\n",
    "        #print(f\"[Error] Failed to retrieve TaxID for {protein_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def fill_taxid_from_protein(row):\n",
    "    if row[\"TaxID\"] is None:\n",
    "        protein_id = row.get(\"prot_name\")\n",
    "        return get_taxid_from_protein_id(protein_id)\n",
    "    else:\n",
    "        return row[\"TaxID\"]\n",
    "\n",
    "def update_taxid_if_default_and_valid_protein(row):\n",
    "    if row[\"TaxID\"] in [\"1\", \"2\", \"131567\"]:\n",
    "        prot = row.get(\"prot_name\")\n",
    "        if isinstance(prot, str) and prot and \" \" not in prot:\n",
    "            new_taxid = get_taxid_from_protein_id(prot)\n",
    "            return new_taxid if new_taxid else row[\"TaxID\"]\n",
    "    return row[\"TaxID\"]\n",
    "\n",
    "\n",
    "def fill_taxid_from_virus_pattern(row, virus_dict):\n",
    "    if pd.notna(row[\"TaxID\"]):\n",
    "        return row[\"TaxID\"]  # Already filled\n",
    "\n",
    "    prot = row.get(\"prot_name\")\n",
    "    if not isinstance(prot, str):\n",
    "        return None\n",
    "\n",
    "    for virus_name, taxid in virus_dict.items():\n",
    "        if virus_name in prot:\n",
    "            return taxid\n",
    "\n",
    "    return None\n",
    "\n",
    "def extract_mnemonic_from_uniref(uniref_func, return_full=False):\n",
    "    \"\"\"\n",
    "    Extract the RepID or mnemonic (e.g., 9FIRM) from a uniref_func string.\n",
    "\n",
    "    - If RepID has an underscore, return:\n",
    "        - full RepID if return_full=True\n",
    "        - only part after '_' if return_full=False\n",
    "    - If RepID does NOT have an underscore, always return full RepID.\n",
    "    \"\"\"\n",
    "    if isinstance(uniref_func, str):\n",
    "        match = re.search(r'RepID=(\"?)([^\\s\"]+)\\1', uniref_func)\n",
    "        if match:\n",
    "            full_id = match.group(2)\n",
    "            if \"_\" in full_id:\n",
    "                if return_full:\n",
    "                    return full_id\n",
    "                else:\n",
    "                    return full_id.split(\"_\", 1)[1]\n",
    "            else:\n",
    "                return full_id\n",
    "    return None\n",
    "\n",
    "def get_taxid_from_mnemonic_uniprot(mnemonic):\n",
    "    \"\"\"\n",
    "    Look up taxonomic ID from a UniProt mnemonic (e.g. '9FIRM') using the UniProt API.\n",
    "    \"\"\"\n",
    "    url = f\"https://rest.uniprot.org/taxonomy/search\"\n",
    "    params = {\n",
    "        \"query\": f\"mnemonic:{mnemonic}\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if data.get(\"results\"):\n",
    "            tax_id = data[\"results\"][0].get(\"taxonId\")\n",
    "            return str(tax_id)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] UniProt lookup failed for {mnemonic}: {e}\")\n",
    "\n",
    "    return None"
   ],
   "id": "39aa5f5414b3e163",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T14:47:21.603274Z",
     "start_time": "2025-07-05T14:47:21.580899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def annotate_taxonomy(\n",
    "    df,\n",
    "    organism_col=None,\n",
    "    rep_id_col=None,\n",
    "    prot_id_col=None,\n",
    "    taxid_col=None,\n",
    "    method_priority=(\"organism\", \"rep_id\", \"prot_id\", \"tax_id\"),  # explicit order\n",
    "    outfile_path=\"taxonomy_output.csv\",\n",
    "    max_rows=None\n",
    "):\n",
    "    lineage_fields = ['domain', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species', 'common']\n",
    "    id_cache = {}\n",
    "\n",
    "    # Map method name to column name\n",
    "    method_cols = {\n",
    "        \"organism\": organism_col,\n",
    "        \"rep_id\": rep_id_col,\n",
    "        \"prot_id\": prot_id_col,\n",
    "        \"tax_id\": taxid_col\n",
    "    }\n",
    "\n",
    "    with open(outfile_path, \"w\", newline=\"\", encoding=\"utf-8\") as out_f:\n",
    "        writer = csv.DictWriter(\n",
    "            out_f,\n",
    "            fieldnames=[\"peptide_name\"] + lineage_fields\n",
    "        )\n",
    "        writer.writeheader()\n",
    "\n",
    "        if max_rows:\n",
    "            df = df.head(max_rows)\n",
    "\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Fetching taxonomy\"):\n",
    "            row_data = dict.fromkeys(lineage_fields, \"\")\n",
    "            lineage = {}\n",
    "\n",
    "            # ----- Prioritization logic -----\n",
    "            for method in method_priority:\n",
    "                col = method_cols.get(method)\n",
    "                if not col or col not in row:\n",
    "                    continue\n",
    "                val = row[col]\n",
    "                if pd.isna(val):\n",
    "                    continue\n",
    "\n",
    "                if method == \"organism\" and pd.notna(val) and isinstance(val, str) and val.strip(): #if method == \"organism\" and str(val).strip() and val is not pd.NA:\n",
    "                    if val in id_cache:\n",
    "                        tax_id = id_cache[val]\n",
    "                        if tax_id in id_cache:\n",
    "                            lineage = id_cache[tax_id]\n",
    "                        else:\n",
    "                            lineage = get_lineage_from_taxid(tax_id) or {}\n",
    "                            id_cache[tax_id] = lineage\n",
    "                    else:\n",
    "                        tax_id = search_taxid_by_name(val)\n",
    "                        if tax_id:\n",
    "                            id_cache[val] = tax_id\n",
    "                            lineage = get_lineage_from_taxid(tax_id) or {}\n",
    "                            id_cache[tax_id] = lineage\n",
    "\n",
    "                elif method == \"rep_id\" and pd.notna(val):\n",
    "                    rep_data = lookup_organism(val)\n",
    "                    row_data[\"common\"] = rep_data.get(\"common\", \"\")\n",
    "                    tax_id = rep_data.get(\"tax_id\")\n",
    "                    if tax_id:\n",
    "                        if tax_id in id_cache:\n",
    "                            lineage = id_cache[tax_id]\n",
    "                        else:\n",
    "                            lineage = get_lineage_from_taxid(tax_id) or {}\n",
    "                            id_cache[tax_id] = lineage\n",
    "\n",
    "                elif method == \"prot_id\" and pd.notna(val):\n",
    "                    if val in id_cache:\n",
    "                        tax_id = id_cache[val]\n",
    "                        if tax_id in id_cache:\n",
    "                            lineage = id_cache[tax_id]\n",
    "                        else:\n",
    "                            lineage = get_lineage_from_taxid(tax_id) or {}\n",
    "                            id_cache[tax_id] = lineage\n",
    "                    else:\n",
    "                        tax_id = get_taxid_from_protein_id(val)\n",
    "                        if tax_id:\n",
    "                            id_cache[val] = tax_id\n",
    "                            lineage = get_lineage_from_taxid(tax_id) or {}\n",
    "                            id_cache[tax_id] = lineage\n",
    "\n",
    "                elif method == \"tax_id\" and pd.notna(val):\n",
    "                    tax_id = val\n",
    "                    if tax_id in id_cache:\n",
    "                        lineage = id_cache[tax_id]\n",
    "                    else:\n",
    "                        lineage = get_lineage_from_taxid(tax_id) or {}\n",
    "                        id_cache[tax_id] = lineage\n",
    "\n",
    "                if lineage:\n",
    "                    break\n",
    "            # Write output\n",
    "            for key in lineage_fields:\n",
    "                if key in lineage:\n",
    "                    row_data[key] = lineage[key]\n",
    "\n",
    "            if not any(row_data.values()):\n",
    "                continue\n",
    "\n",
    "            writer.writerow({\"peptide_name\": idx, **row_data})\n",
    "            out_f.flush()"
   ],
   "id": "e117becbdcbb9bc2",
   "outputs": [],
   "execution_count": 340
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rep_data = lookup_organism(\"R6BPJ2\")  # Human hemoglobin\n",
    "rep_data"
   ],
   "id": "fa65deaf53fbe814",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# AGILENT LIBRARY",
   "id": "66adddd4679a8adc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Using uniref ids directly to get species\n",
    "df = pd.read_csv(\"agilent_library_with_info_extended.csv\", index_col=0).head(10)\n",
    "df['uniref'].apply(lookup_organism)"
   ],
   "id": "e8648f8e8ce3051c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_pickle(\"/home/creyna/Vogl-lab_Projects_git/HCC_MUW_analysis/Data/agilent_library_with_info.pkl\")\n",
    "df = df[[\"uniref\", \"uniref_func\", \"Organism_complete_name\",\"IEDB_organism_name\"]]\n",
    "# Remove the prefix only if it's present at the start\n",
    "df[\"uniref\"] = df[\"uniref\"].str.replace(r\"^UniRef90_\", \"\", regex=True)\n",
    "df\n",
    "#df.to_csv(\"/home/creyna/Vogl-lab_Projects_git/HCC_MUW_analysis/Data/agilent_library_with_info_uniref_organism.csv\")"
   ],
   "id": "6af46986d11c1f7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Predefine lineage fields of interest\n",
    "lineage_fields = ['domain', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species', 'common']\n",
    "\n",
    "# Initialize empty list to store results\n",
    "lineage_data = []\n",
    "organism_cache = {}\n",
    "\n",
    "\n",
    "with open(\"/home/creyna/Vogl-lab_Projects_git/HCC_MUW_analysis/Data/2022_agilent_library_with_info_extended_annotation.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as out_f:\n",
    "    writer = csv.DictWriter(\n",
    "        out_f,\n",
    "        fieldnames=[\"peptide_name\"] + lineage_fields,  # or whatever columns you need alongside your original df cols\n",
    "    )\n",
    "    writer.writeheader()\n",
    "\n",
    "    for idx, row in tqdm(df.head(10).iterrows(), total=len(df), desc=\"Fetching taxonomy\"):\n",
    "        row_data = dict.fromkeys(lineage_fields, \"\")\n",
    "        rep_id   = row[\"uniref\"]\n",
    "        sci_name = row[\"Organism_complete_name\"]\n",
    "\n",
    "        # First, attempt UniProt ID lookup\n",
    "        rep_data = {}\n",
    "        if isinstance(rep_id, str) and rep_id.strip():\n",
    "            rep_data = lookup_organism(rep_id)\n",
    "        row_data[\"common\"] = rep_data.get(\"common\", \"\")\n",
    "\n",
    "        # If no tax_id yet, try scientific name lookup\n",
    "        if not rep_data.get(\"tax_id\")  and isinstance(sci_name, str) and sci_name.strip():\n",
    "\n",
    "            tax_id = search_taxid_by_name(sci_name)\n",
    "            organism_cache[sci_name] = tax_id\n",
    "\n",
    "            if tax_id:\n",
    "                rep_data = {\"common\":   row_data[\"common\"] , \"scientific\": sci_name, \"tax_id\": tax_id}\n",
    "\n",
    "        # If still no tax_id, skip\n",
    "        if not rep_data.get(\"tax_id\"):\n",
    "            lineage_data.append(row_data)\n",
    "            continue\n",
    "\n",
    "        # fetch lineage by TaxID\n",
    "        tax_id = rep_data[\"tax_id\"]\n",
    "        if tax_id in organism_cache:\n",
    "            lineage = organism_cache[tax_id]\n",
    "        else:\n",
    "            lineage = get_lineage_from_taxid(tax_id) or {}\n",
    "            organism_cache[tax_id] = lineage\n",
    "\n",
    "        for key in lineage_fields:\n",
    "            if key in lineage:\n",
    "                row_data[key] = lineage[key]\n",
    "        #lineage_data.append(row_data)\n",
    "        # include the original row index if you want\n",
    "        row_to_write = {\"peptide_name\": idx}\n",
    "        row_to_write.update(row_data)\n",
    "        writer.writerow(row_to_write)"
   ],
   "id": "57a556d15ea2867f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TWIST Library",
   "id": "7d1bdc9484bd578d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#df = pd.read_csv(\"twist_library_with_info_extended.csv\", index_col=0).head(10)\n",
    "#df = pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/HCC_MUW_analysis/Data/twist_library_with_info.csv\", index_col=0, low_memory=False)\n",
    "#df['allergome_uniprot'].apply(lookup_organism)\n",
    "\n",
    "# Apply cleaning function\n",
    "#df['Organism_complete_name'] = df.apply(get_organism_complete_name, axis=1)\n",
    "#df.to_csv(\"/home/creyna/Vogl-lab_Projects_git/HCC_MUW_analysis/Data/twist_library_with_info_cleanNames.csv\")\n",
    "#phage_name\n",
    "# Columns in priority order\n",
    "#rep_cols = ['allergome_uniprot', 'allergen_uniprot', 'iedb_uniprot', 'fummy_uniprot', 'gened_uniprot']\n",
    "\n",
    "# Apply across the prioritized columns\n",
    "#df['rep_id'] = df.apply(collect_valid_ids, axis=1)\n",
    "#df.to_csv(\"/home/creyna/Vogl-lab_Projects_git/HCC_MUW_analysis/Data/twist_library_with_info_cleanNames_repIDs.csv\")\n",
    "df = pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/HCC_MUW_analysis/Data/twist_library_with_info_cleanNames_repIDs.csv\", index_col=0, low_memory=False)\n",
    "#df.iloc[:,[3,58,59]].to_csv(\"/home/creyna/Vogl-lab_Projects_git/HCC_MUW_analysis/Data/twist_library_with_important_info_cleanNames_repIDs.csv\")"
   ],
   "id": "2820a1bf293d5490"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Predefine lineage fields of interest\n",
    "lineage_fields = ['domain', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species', 'common']\n",
    "lineage_data = []\n",
    "organism_cache = {}\n",
    "\n",
    "with open(\"/home/creyna/Vogl-lab_Projects_git/HCC_MUW_analysis/Data/2_twist_library_with_info_extended_annotation.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as out_f:\n",
    "    writer = csv.DictWriter(\n",
    "        out_f,\n",
    "        fieldnames=[\"peptide_name\"] + lineage_fields,  # or whatever columns you need alongside your original df cols\n",
    "    )\n",
    "    writer.writeheader()\n",
    "\n",
    "    for idx, row in tqdm(df.head(10).iterrows(), total=len(df), desc=\"Fetching taxonomy\"):\n",
    "        row_data = dict.fromkeys(lineage_fields, \"\")\n",
    "        rep_id   = row[\"rep_id\"]\n",
    "        sci_name = row[\"Organism_complete_name\"]\n",
    "\n",
    "        # First, attempt UniProt ID lookup\n",
    "        rep_data = {}\n",
    "        if isinstance(rep_id, str) and rep_id.strip():\n",
    "            rep_data = lookup_organism(rep_id)\n",
    "        row_data[\"common\"] = rep_data.get(\"common\", \"\")\n",
    "\n",
    "        # If no tax_id yet, try scientific name lookup\n",
    "        if not rep_data.get(\"tax_id\")  and isinstance(sci_name, str) and sci_name.strip():\n",
    "\n",
    "            tax_id = search_taxid_by_name(sci_name)\n",
    "            organism_cache[sci_name] = tax_id\n",
    "\n",
    "            if tax_id:\n",
    "                rep_data = {\"common\":   row_data[\"common\"] , \"scientific\": sci_name, \"tax_id\": tax_id}\n",
    "\n",
    "        # If still no tax_id, skip\n",
    "        if not rep_data.get(\"tax_id\"):\n",
    "            lineage_data.append(row_data)\n",
    "            continue\n",
    "\n",
    "        # fetch lineage by TaxID\n",
    "        tax_id = rep_data[\"tax_id\"]\n",
    "        if tax_id in organism_cache:\n",
    "            lineage = organism_cache[tax_id]\n",
    "        else:\n",
    "            lineage = get_lineage_from_taxid(tax_id) or {}\n",
    "            organism_cache[tax_id] = lineage\n",
    "\n",
    "        for key in lineage_fields:\n",
    "            if key in lineage:\n",
    "                row_data[key] = lineage[key]\n",
    "        #lineage_data.append(row_data)\n",
    "        # include the original row index if you want\n",
    "        row_to_write = {\"peptide_name\": idx}\n",
    "        row_to_write.update(row_data)\n",
    "        writer.writerow(row_to_write)"
   ],
   "id": "242e6eb02e68f7cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CORONA",
   "id": "923de6ca3ff64b21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/HCC_MUW_analysis/Data/corona2_library_with_info.csv\", index_col=0, low_memory=False)\n",
    "df = df[['Description', 'Uniprot', 'virus_name']].rename(columns={\n",
    "    'Uniprot': 'rep_id',\n",
    "    'virus_name': 'Organism_complete_name'\n",
    "})\n",
    "df.to_csv(\"/home/creyna/Vogl-lab_Projects_git/HCC_MUW_analysis/Data/corona2_library_with_important_info.csv\")"
   ],
   "id": "b13692fadda87a5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Predefine lineage fields of interest\n",
    "lineage_fields = ['domain', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species', 'common']\n",
    "lineage_data = []\n",
    "organism_cache = {}\n",
    "\n",
    "with open(\"/home/creyna/Vogl-lab_Projects_git/HCC_MUW_analysis/Data/2_corona2_library_with_info_extended_annotation.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as out_f:\n",
    "    writer = csv.DictWriter(\n",
    "        out_f,\n",
    "        fieldnames=[\"peptide_name\"] + lineage_fields,  # or whatever columns you need alongside your original df cols\n",
    "    )\n",
    "    writer.writeheader()\n",
    "\n",
    "    for idx, row in tqdm(df.head(10).iterrows(), total=len(df), desc=\"Fetching taxonomy\"):\n",
    "        row_data = dict.fromkeys(lineage_fields, \"\")\n",
    "        rep_id   = row[\"rep_id\"]\n",
    "        sci_name = row[\"Organism_complete_name\"]\n",
    "\n",
    "        # First, attempt UniProt ID lookup\n",
    "        rep_data = {}\n",
    "        if isinstance(rep_id, str) and rep_id.strip():\n",
    "            rep_data = lookup_organism(rep_id)\n",
    "        row_data[\"common\"] = rep_data.get(\"common\", \"\")\n",
    "\n",
    "        # If no tax_id yet, try scientific name lookup\n",
    "        if not rep_data.get(\"tax_id\")  and isinstance(sci_name, str) and sci_name.strip():\n",
    "\n",
    "            tax_id = search_taxid_by_name(sci_name)\n",
    "            organism_cache[sci_name] = tax_id\n",
    "\n",
    "            if tax_id:\n",
    "                rep_data = {\"common\":   row_data[\"common\"] , \"scientific\": sci_name, \"tax_id\": tax_id}\n",
    "\n",
    "        # If still no tax_id, skip\n",
    "        if not rep_data.get(\"tax_id\"):\n",
    "            lineage_data.append(row_data)\n",
    "            continue\n",
    "\n",
    "        # fetch lineage by TaxID\n",
    "        tax_id = rep_data[\"tax_id\"]\n",
    "        if tax_id in organism_cache:\n",
    "            lineage = organism_cache[tax_id]\n",
    "        else:\n",
    "            lineage = get_lineage_from_taxid(tax_id) or {}\n",
    "            organism_cache[tax_id] = lineage\n",
    "\n",
    "        for key in lineage_fields:\n",
    "            if key in lineage:\n",
    "                row_data[key] = lineage[key]\n",
    "        #lineage_data.append(row_data)\n",
    "        # include the original row index if you want\n",
    "        row_to_write = {\"peptide_name\": idx}\n",
    "        row_to_write.update(row_data)\n",
    "        writer.writerow(row_to_write)"
   ],
   "id": "fce53b6ffa410fca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d0367e38b7106333"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Automated lineage annotation",
   "id": "52d6b878572f3f9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "annotate_taxonomy(\n",
    "    df=df,\n",
    "    organism_col=\"IEDB_organism_name\",\n",
    "    taxid_col=\"TaxID\",\n",
    "    #rep_id_col=\"rep_id\",\n",
    "    method_priority=[\"organism\", \"tax_id\"],\n",
    "    #prioritize=\"organism\",\n",
    "    outfile_path=\"/home/creyna/Vogl-lab_Projects_git/HCC_MUW_analysis/Data/output_organism_first.csv\",\n",
    "    max_rows=500\n",
    ")"
   ],
   "id": "df6cb1908e98035f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Curate Agilent using Protein Acession or other name fields",
   "id": "5919107b0f01857a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Agilent missing",
   "id": "5645addf701f4310"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_all = pd.read_pickle(\"/home/creyna/Vogl-lab_Projects_git/HCC_MUW_analysis/Data/agilent_library_with_info.pkl\")",
   "id": "965b5f6108adfe21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Recover TaxID from other sources",
   "id": "f735b10118b059cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "manual_taxids_agilent = {\"Bacteroides dorei CL03T12C01\":\"997877\",\n",
    " \"Lactobacillus reuteri DSM 20016\": \"557436\",\n",
    " \"Bacteroides vulgatus ATCC 8482\": \"435590\",\n",
    " \"Faecalibacterium prausnitzii A2-165\": \"411483\",\n",
    " \"Roseburia inulinivorans DSM 16841\":\"622312\",\n",
    " \"Bifidobacterium longum 35624\":\"205913\",\n",
    " \"Ruminococcus gnavus AGR2154\":\"1384063\",\n",
    " \"Lactobacillus rhamnosus GG\":\"568703\",\n",
    " \"Bacteroides uniformis ATCC 8492\":\"411479\",\n",
    " \"[Eubacterium] hallii DSM 3353\":\"411469\",\n",
    " \"Lactobacillus plantarum WCFS1\":\"220668\",\n",
    " \"Bacteroides ovatus\":\"665954\",\n",
    " \"Dorea longicatena DSM 13814\":\"411462\",\n",
    " \"Bifidobacterium breve UCC2003\":\"326426\",\n",
    " \"Streptococcus lutetiensis 033\":\"1076934\",\n",
    " \"Streptococcus pyogenes MGAS8232\":\"186103\",\n",
    " \"Salmonella enterica subsp. enterica serovar Enteritidis str. P125109\":\"550537\",\n",
    " \"Escherichia coli O157:H7 str. Sakai\":\"386585\",\n",
    " \"Blautia obeum ATCC 29174\":\"411459\",\n",
    " \"Escherichia coli Nissle 1917\":\"316435\",\n",
    " \"Escherichia coli CFT073\":\"199310\",\n",
    " \"Enterococcus faecalis str. MMH594\":\"1351\",\n",
    " \"Escherichia coli O157:H7 str. EDL933\":\"155864\",\n",
    " \"Enterococcus faecalis V583\":\"226185\",\n",
    " \"Shigella flexneri 2a str. 301\":\"198214\"}\n",
    "\n",
    "manual_virus_agilent = {'Enterovirus A':\"138948\",\n",
    " 'Enterovirus B':\"138949\",\n",
    " 'Enterovirus C':\"138950\",\n",
    " 'Human adenovirus C':\"129951\",\n",
    " 'Human herpesvirus 1':\"10298\",\n",
    " 'Human herpesvirus 3':\"10335\",\n",
    " 'Human herpesvirus 4':\"10377\",\n",
    " 'Human herpesvirus 5':\"10360\",\n",
    " 'Human immunodeficiency virus 1':\"11676\",\n",
    " 'Human respiratory syncytial virus':\"410078\",\n",
    " 'Influenza A virus':\"384505\",\n",
    " 'Norwalk virus':\"11983\",\n",
    " 'Rhinovirus B':\"147712\"}\n",
    "\n",
    "df = pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/rerun_missing_agilent_peptide_annotations.csv\", index_col=0, low_memory=False)\n",
    "# clean IEDB col names, remove unnecesary things\n",
    "df['IEDB_organism_name'] = df.apply(get_organism_complete_name, axis=1)\n",
    "# following three need to be manually corrected\n",
    "df.loc[df['Organism_complete_name'] == \"Terrapene carolina triunguis\", 'IEDB_organism_name'] = \"Terrapene triunguis\"\n",
    "df.loc[df['IEDB_organism_name'] == \"Human papillomavirus type 16\", 'IEDB_organism_name']  = \"Alphapapillomavirus 9\"\n",
    "df.loc[df['Organism_complete_name'] == \"Orycteropus afer afer\", 'IEDB_organism_name'] = \"Orycteropus afer\"\n",
    "#get taxid from uniref_func content\n",
    "df['TaxID'] = df['uniref_func'].apply(extract_taxid)\n",
    "# update missing TaxID values based on manual dict\n",
    "df.loc[df[\"Organism_complete_name\"].isin(manual_taxids_agilent.keys()), \"TaxID\"] = df.loc[df[\"Organism_complete_name\"].isin(manual_taxids_agilent.keys()), \"Organism_complete_name\"].map(manual_taxids_agilent)\n",
    "#df.to_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/rerun_missing_agilent_peptide_annotations_addTaxIDcol.csv\")\n",
    "#df = pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/HCC_MUW_analysis/Data/final_missing_agilent.csv\", index_col=0, low_memory=False)\n",
    "\n",
    "# some TaxID values are still missing, add prot_name col as extra info to get taxID\n",
    "df =df.merge(df_all.loc[:, [\"prot_name\"]], how=\"left\", left_index=True, right_index=True)\n",
    "#df[\"TaxID\"] = df[\"TaxID\"].apply(lambda x: str(int(x)) if pd.notna(x) else None)\n",
    "# clean prot_name\n",
    "df[\"prot_name\"] = df[\"prot_name\"].apply(clean_prot_name_from_vfg)\n",
    "# Update missing TaxID values based on virus patterns in prot_name\n",
    "df.loc[df[\"TaxID\"].isna(), \"TaxID\"] = df.loc[df[\"TaxID\"].isna()].apply(lambda row: fill_taxid_from_virus_pattern(row, manual_virus_agilent), axis=1)"
   ],
   "id": "bf034268e38fd761",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# use protein id to get TaxID, only apply to missing TaxID rows\n",
    "df.loc[df[\"TaxID\"].isna(), \"TaxID\"] = df.loc[df[\"TaxID\"].isna()].apply(lambda row: (\n",
    "    None if pd.isna(row.get(\"prot_name\")) or\n",
    "            not isinstance(row.get(\"prot_name\"), str) or\n",
    "            \" \" in row[\"prot_name\"]\n",
    "    else get_taxid_from_protein_id(row[\"prot_name\"])\n",
    "), axis=1)\n",
    "# Update taxID if values are 1 or 2 or  and if prot_name is not None and valid\n",
    "df.loc[df[\"TaxID\"].isin([\"1\", \"2\", \"131567\"]), \"TaxID\"] = df.loc[df[\"TaxID\"].isin([\"1\", \"2\", \"131567\"])].apply(update_taxid_if_default_and_valid_protein, axis=1)\n",
    "#df.to_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/rerun_missing_agilent_peptide_annotations_cleaned_addTaxIDcol.csv\")"
   ],
   "id": "26457b4d5a5530c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Try to get TaxID from repID",
   "id": "6ff1178fdc21cef4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "manual_repid_agilent = {\"BACVU\":\"702446\", \"9BACT\":\"165179\"}\n",
    "# try to get RepID and with that TaxID, less accurate\n",
    "toUse_repID_df = df.loc[(df[\"TaxID\"].isin([\"1\", \"2\", \"131567\"])) & (df['Organism_complete_name'].isin([\"root\", \"Bacteria\", \"cellular organisms\"]))]\n",
    "toUse_repID_df[\"repID\"] = toUse_repID_df[\"uniref_func\"].apply(lambda x: extract_mnemonic_from_uniref(x, return_full=False))\n",
    "#toUse_repID_df[\"repID_mnemonic\"] = toUse_repID_df[\"uniref_func\"].apply(lambda x: extract_mnemonic_from_uniref(x, return_full=False))\n",
    "toUse_repID_df['TaxID'] = toUse_repID_df['repID'].apply(get_taxid_from_mnemonic_uniprot)\n",
    "# Update taxID if values are 2 or NaN with manual mapping\n",
    "toUse_repID_df.loc[toUse_repID_df[\"repID\"].isin(manual_repid_agilent.keys()), \"TaxID\"] = (toUse_repID_df[\"repID\"].map(manual_repid_agilent))"
   ],
   "id": "49ccaa4b2e2ac29e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main Curated Agilent to get Lineages",
   "id": "57cfbcf3355881cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df[\"TaxID\"].update(toUse_repID_df[\"TaxID\"])\n",
    "df[\"TaxID\"] = df[\"TaxID\"].apply(lambda x: str(int(x)) if pd.notna(x) else None)\n",
    "df.to_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/rerun_missing_agilent_peptide_annotations_cleaned_addTaxIDcol.csv\")"
   ],
   "id": "56b6b1fbcbb505c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b3c2f16c60dc3647"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## No TaxID for these ones",
   "id": "fb9432e41d44939c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# No TaxID\n",
    "miss_df =df.loc[df[\"TaxID\"].isna()].merge(df_all, how=\"left\", left_index=True, right_index=True) #df[\"TaxID\"] = df[\"TaxID\"].apply(lambda x: str(int(x)) if pd.notna(x) else None)\n",
    "miss_df.shape"
   ],
   "id": "d40fe142b40a38cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## These peptides have TaxID 1 or 2 or no lineage based on UniProt iD",
   "id": "bd2f30add66577b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/rerun_missing_agilent_asBacteriaOrempty.csv\", index_col=0, low_memory=False)\n",
    "df['TaxID'] = df['uniref_func'].apply(extract_taxid)\n",
    "df =df.merge(df_all.loc[:, [\"prot_name\"]], how=\"left\", left_index=True, right_index=True)\n",
    "df[\"prot_name\"] = df[\"prot_name\"].apply(clean_prot_name_from_vfg)\n",
    "df['TaxID'] = df.apply(update_taxid_if_default_and_valid_protein, axis=1)\n",
    "\n",
    "toUse_repID_df = df.loc[(df[\"TaxID\"].isin([\"1\", \"2\", \"131567\"])) & (df['Organism_complete_name'].isin([\"root\", \"Bacteria\", \"cellular organisms\"]))]\n",
    "toUse_repID_df[\"repID\"] = toUse_repID_df[\"uniref_func\"].apply(lambda x: extract_mnemonic_from_uniref(x, return_full=False))\n",
    "#toUse_repID_df[\"repID_mnemonic\"] = toUse_repID_df[\"uniref_func\"].apply(lambda x: extract_mnemonic_from_uniref(x, return_full=False))\n",
    "toUse_repID_df['TaxID'] = toUse_repID_df['repID'].apply(get_taxid_from_mnemonic_uniprot)\n",
    "# Update taxID if values are 2 or NaN with manual mapping\n",
    "toUse_repID_df.loc[toUse_repID_df[\"repID\"].isin(manual_repid_agilent.keys()), \"TaxID\"] = (toUse_repID_df[\"repID\"].map(manual_repid_agilent))\n",
    "# In case some fail due to network issues\n",
    "toUse_repID_df.loc[toUse_repID_df[\"TaxID\"].isna(), \"TaxID\"] = toUse_repID_df.loc[toUse_repID_df[\"TaxID\"].isna()]['repID'].apply(get_taxid_from_mnemonic_uniprot)\n",
    "\n",
    "df[\"TaxID\"].update(toUse_repID_df[\"TaxID\"])\n",
    "df[\"TaxID\"] = df[\"TaxID\"].apply(lambda x: str(int(x)) if pd.notna(x) else None)\n",
    "df.to_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/rerun_missing_agilent_asBacteriaOrempty_cleaned_addTaxIDcol.csv\")"
   ],
   "id": "8af2b127cdb68968",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Final Combined Curated Agilent to get Lineages\n",
    "\n",
    "257 do not have info to recover some lineage info"
   ],
   "id": "f5693302b771f3df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pd.concat([pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/rerun_missing_agilent_peptide_annotations_cleaned_addTaxIDcol.csv\", index_col=0, low_memory=False),\n",
    "           df], axis=0).to_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/rerun_missing_agilent_combined_peptide_annotations_cleaned_addTaxIDcol.csv\")"
   ],
   "id": "5cdceba2c0467fbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### clean full agilent metadata to add new final agilen annotations",
   "id": "b1a5e3f77f9f9ff6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_uniref_description(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    # Remove anything starting from ' n=' or ' Key=' (e.g. Tax=, RepID=, etc.)\n",
    "    return re.split(r'\\s+(?:n=|Tax=|RepID=|TaxID=)', val)[0].strip()\n",
    "\n",
    "# Apply it and create new column 'Description'\n",
    "df_all[\"Description\"] = df_all[\"uniref_func\"].apply(clean_uniref_description)\n",
    "tmp_all = df_all[[\"aa_seq\", \"pos\", \"len_seq\",\"full name\",\"Description\",\"is_IEDB_or_cntrl\",\"is_auto\",\"is_infect\",\"is_EBV\",\"is_toxin\",\"is_PNP\",\"is_EM\",\"is_MPA\",\"is_patho\",\"is_probio\",\"is_IgA\",\"is_flagellum\",\"signalp6_slow\",\"is_topgraph_new\"]]\n",
    "tmp_all[\"aa_seq\"] = tmp_all[\"aa_seq\"].str.replace(r\"\\(.*\", \"\", regex=True).str.strip()\n",
    "bool_cols = [\"is_flagellum\", \"signalp6_slow\", \"is_topgraph_new\"]\n",
    "tmp_all[bool_cols] = tmp_all[bool_cols].astype(bool)\n",
    "tmp_annot = pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/agilent_completeAnnotation_library_with_lineages.csv\", index_col=0, low_memory=False)\n",
    "pd.merge(tmp_all, tmp_annot, how=\"left\", left_index=True, right_index=True).to_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/agilent_library_with_lineages_important_info.csv\")"
   ],
   "id": "97d59ae19d2c398",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Curate Corona Library",
   "id": "9b4f3486b1d2377c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# everything having pdb| is sars-cov-2 2697049\n",
    "#\"Severe acute respiratory syndrome coronavirus 2\" is sars-cov-2 2697049"
   ],
   "id": "9475bb7cdada4a3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T16:41:26.375229Z",
     "start_time": "2025-07-04T16:41:26.359542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_and_patch_df(df):\n",
    "    # Step 0: Ensure required columns exist\n",
    "    if \"TaxID\" not in df.columns:\n",
    "        df[\"TaxID\"] = pd.NA\n",
    "\n",
    "    # Step 1: Set organism name and TaxID for SARS-CoV-2 and pdb| cases\n",
    "    sars_mask = df['Organism_complete_name'].str.contains(\"Severe acute respiratory syndrome coronavirus 2\", na=False)\n",
    "    pdb_mask = df['Organism_complete_name'].str.contains(\"pdb\\\\|\", na=False)\n",
    "    combined_mask = sars_mask | pdb_mask\n",
    "\n",
    "    df.loc[combined_mask, \"Organism_complete_name\"] = \"Severe acute respiratory syndrome coronavirus 2\"\n",
    "    df.loc[combined_mask, \"TaxID\"] = \"2697049\"\n",
    "\n",
    "    # Step 2: Clean `prot_id` → keep only first entry before `&`, and remove suffixes like `_NSP13`\n",
    "    df['prot_id'] = (\n",
    "        df['prot_id']\n",
    "        .astype(str)\n",
    "        .str.split('&').str[0]\n",
    "        .str.extract(r\"^([A-Z0-9_]+(?:\\.\\d+)?)\")[0]\n",
    "    )\n",
    "\n",
    "    # Step 3: Clean `prot_name` → keep only first part if multiple descriptions separated by '&'\n",
    "    if 'prot_name' in df.columns:\n",
    "        df['prot_name'] = df['prot_name'].astype(str).str.split('&').str[0].str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/missing_corona_peptide_annotations.csv\", index_col=0, low_memory=False)\n",
    "df_all = pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/corona2_library_with_info.csv\", index_col=0, low_memory=False)\n",
    "df = pd.merge(df, df_all[[\"prot_id\", \"prot_name\"]], how=\"left\", left_index=True, right_index=True)\n",
    "df = clean_and_patch_df(df)\n",
    "df.to_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/missing_corona_peptide_annotations_cleaned_addTaxIDcol.csv\")"
   ],
   "id": "3aa02f5007ab9d5f",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T21:50:55.603637Z",
     "start_time": "2025-07-04T21:50:55.599834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_all.rename(columns={\"virus_name\": \"Organism_complete_name\"}, inplace=True)\n",
    "df_all = clean_and_patch_df(df_all)\n",
    "tmp_all = df_all[[\"aa_seq\",\"pos\",\"len_seq\",\"full name\",\"prot_name\"]]\n",
    "tmp_all[\"aa_seq\"] = tmp_all[\"aa_seq\"].str.replace(r\"\\(.*\", \"\", regex=True).str.strip()\n",
    "tmp_all.rename(columns={\"prot_name\": \"Description\"}, inplace=True)\n",
    "tmp_annot = pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/corona_completeAnnotation_library_with_lineages.csv\", index_col=0, low_memory=False)\n",
    "pd.merge(tmp_all, tmp_annot, how=\"left\", left_index=True, right_index=True).to_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/corona2_library_with_lineages_important_info.csv\")"
   ],
   "id": "f0e31e1b07a004ad",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Curate TWIST",
   "id": "95f7c2c338d6ddd0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:05:41.369831Z",
     "start_time": "2025-07-05T17:05:40.148053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "manual_twist_virus = {'Enterovirus A':\"138948\",\n",
    " 'Enterovirus B':\"138949\",\n",
    " 'Enterovirus C':\"138950\",\n",
    " 'Human adenovirus C':\"129951\",\n",
    " 'Human herpesvirus 1':\"10298\",\n",
    " 'Human herpesvirus 3':\"10335\",\n",
    " 'Human herpesvirus 4':\"10377\",\n",
    " 'Human herpesvirus 5':\"10360\",\n",
    " 'Human immunodeficiency virus 1':\"11676\",\n",
    " 'Human respiratory syncytial virus':\"410078\",\n",
    " 'Influenza A virus':\"384505\",\n",
    " 'Norwalk virus':\"11983\",\n",
    " 'Rhinovirus B':\"147712\",\n",
    " 'Bundibugyo ebolavirus':\"565995\",\n",
    " 'Sudan ebolavirus':\"186540\",\n",
    " 'SAPK4 (MAPK13)':\"9986\",\n",
    " 'Borna disease virus Giessen strain He/80':\"1714621\",\n",
    " 'Mycoplasma penetrans':\"272633\",\n",
    " 'Haemophilus influenzae NTHi 1479':\"375177\",\n",
    " 'Chikungunya virus MY/08/065':\"37124\",\n",
    " 'Porphyromonas gingivalis OMZ 409':\"242619\",\n",
    " 'Haemophilus influenzae Subtype 1H':\"727\",\n",
    " 'Human rhinovirus A89':\"12132\",\n",
    " 'Entamoeba histolytica YS-27':\"5759\"}\n",
    "\n",
    "mapping_twist_flu={\"H3N2 A\":\"11320\", \"H3N2 B\":\"11520\", \"H1N1\":\"260815\", \"B B/\":\"11520\"}\n",
    "\n",
    "mapping_twist_bac={\"Mycoplasma pneumoniae\":\"2104\", \"Mycoplasma penetrans\":\"28227\"}\n",
    "\n",
    "manual_twist_allergen_name_mapping ={'Hom s Elastin':\"9606\", 'Dan re PGM':\"117571\", 'Mus a 5':\"214687\",\n",
    "       'Sola l PME':\"4081\", 'Str py Streptokinase':\"1314\", 'Hom s TM':\"9606\", 'Dro pp 7':\"46245\",\n",
    "       'Fel d 2':\"9685\", 'Ory la 2':\"8090\", 'Sola l 4':\"4081\", 'Hom s Iduronidase':\"9606\",\n",
    "       'Can f Feld1-like':\"9615\", 'Nas vi 12':\"7425\", 'Mala s 4':\"1230383\", 'Per a 7':\"6978\", 'Gal d 3':\"9031\",\n",
    "       'Tri a 44':\"4565\", 'Mel g 3':\"9103\", 'Dan re 2':\"117571\", 'Bra di 5':\"15368\", 'Asp aw 14':\"1033177\",\n",
    "       'Sola l 7':\"4081\", 'Dan re CK':\"117571\", 'Mus a 3':\"214687\", 'Tri a 42':\"4565\", 'Can f 7':\"9615\",\n",
    "       'Sola l TLP':\"4081\", 'Sola l 5':\"4081\", 'Tri a 45':\"4565\", 'Tri a 41':\"4565\", 'Bomb m 1':\"7091\",\n",
    "       'Mus a 1':\"214687\", 'Bra di 2':\"15368\", 'Tak ru 2':\"31033\", 'Ore ni NDKB':\"8128\", 'Can f 6':\"9615\",\n",
    "       'Asp fl 2':\"332952\", 'Dan re NDKB':\"117571\", 'Acy pi 7':\"133076\", 'Asp aw 3':\"1033177\", 'Nas vi AK':\"7425\",\n",
    "       'Gly m 8':\"3847\", 'Gly m 5':\"3847\", 'Can f 1':\"9615\", 'Can f 8':\"9615\", 'Str dy Streptokinase':\"370554\",\n",
    "       'Equ c 1':\"9796\", 'Mala s 6':\"1230383\", 'Mus a 4':\"214687\", 'Mus a 2':\"214687\", 'Can f 5':\"9615\", 'Hor v 37':\"4513\",\n",
    "       'Hom s PSA':\"9606\", 'Bra di 7':\"15368\", 'Hor v 7k-LTP':\"4513\", 'Fel d 1':\"9685\", 'Mala s 8':\"1230383\",\n",
    "       'Dan re 1':\"117571\", 'Hom s Glucocerebrosidase':\"9606\", 'Mel g 2':\"9103\", 'Sola l SOD':\"4081\"}\n",
    "\n",
    "def first_accession(x):\n",
    "    if isinstance(x, str):\n",
    "        # split on either '&' or ',' (with optional space), take the first piece\n",
    "        return re.split(r'[&,]\\s*', x)[0]\n",
    "    return x\n",
    "\n",
    "df = pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/missing_twist_peptide_annotations.csv\", index_col=0, low_memory=False)\n",
    "df_all = pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/twist_library_with_info.csv\", index_col=0, low_memory=False)\n",
    "df = pd.merge(df, df_all[[\"allergenonline_ncbi\", \"allergome_name\", \"SDAP_name\",\"gened_Accession\",\"gened_comments\", \"IEDB_comments\"]], how=\"left\", left_index=True, right_index=True)\n",
    "df['allergome_name'] = df['allergome_name'].apply(lambda x: re.sub(r\"\\s*\\(.*\", \"\", x) if isinstance(x, str) else x)\n",
    "df['SDAP_name'] = df['SDAP_name'].apply(lambda x: re.sub(r\"\\s*\\(.*\", \"\", x) if isinstance(x, str) else x)\n",
    "#df[\"allergenonline_ncbi\"] = df[\"allergenonline_ncbi\"].apply(lambda x: x.split(\"&\")[0] if isinstance(x, str) else x)\n",
    "df[\"allergenonline_ncbi\"] = df[\"allergenonline_ncbi\"].apply(first_accession)\n",
    "mask = df[\"gened_Accession\"] == \"LTRA_LACLM&LTRA_LACLC\"\n",
    "df.loc[mask, \"TaxID\"] = \"416870\"\n",
    "df.loc[mask, \"gened_Accession\"] = pd.NA\n",
    "valid_mask = (df[\"gened_Accession\"].notna() & (df[\"gened_Accession\"] != False) & (df[\"gened_Accession\"].astype(str).str.strip() != \"\"))\n",
    "df.loc[valid_mask, \"allergenonline_ncbi\"] = df.loc[valid_mask, \"gened_Accession\"] # Update 'allergenonline_ncbi' values with those from 'gened_Accession'\n",
    "df.rename(columns={\"allergenonline_ncbi\": \"prot_id\"}, inplace=True)\n",
    "mask = df['allergome_name'].isin(manual_twist_allergen_name_mapping)\n",
    "df.loc[mask, 'TaxID'] = df.loc[mask, 'allergome_name'].map(manual_twist_allergen_name_mapping)\n",
    "\n",
    "# manual mapping\n",
    "df.loc[df['rep_id'] == \"A0A0A3ZKQ1\", 'TaxID'] = \"556\"\n",
    "\n",
    "mask_deep = df['full name'].str.contains('Deep-sea thermophilic phage D6E', case=False, na=False)\n",
    "df.loc[mask_deep, 'TaxID'] = '749413'\n",
    "mask_deep = df['full name'].str.contains('Human serum albumin', case=False, na=False)\n",
    "df.loc[mask_deep, 'TaxID'] = '9606'\n",
    "# Rows whose comments mention Lactococcus lactis → TaxID = 1358\n",
    "mask_lac = df['gened_comments'].str.contains('Lactococcus lactis', case=False, na=False)\n",
    "df.loc[mask_lac, 'TaxID'] = '1358'\n",
    "# Rows whose comments contain \"From Anat \" → TaxID = 9606\n",
    "mask_anat = df['IEDB_comments'].str.contains(r'\\bFrom Anat\\b', case=False, na=False)\n",
    "df.loc[mask_anat, 'TaxID'] = '9606'\n",
    "\n",
    "# map the flu virus to taxid\n",
    "pattern = r\"^(\" + \"|\".join(re.escape(k) for k in mapping_twist_flu) + r\")\"\n",
    "mask = df[\"full name\"].str.match(pattern, na=False)\n",
    "df.loc[mask, \"prefix\"] = df.loc[mask, \"full name\"].str.extract(pattern)[0]\n",
    "df.loc[mask, \"TaxID\"] = df.loc[mask, \"prefix\"].map(mapping_twist_flu)\n",
    "df.drop(columns=\"prefix\", inplace=True)\n",
    "\n",
    "# map other viruses\n",
    "pattern = r'(' + '|'.join(re.escape(k) for k in manual_twist_virus) + r')'\n",
    "matched = df['full name'].str.extract(pattern, expand=False)\n",
    "mask = matched.notna()\n",
    "df.loc[mask, 'TaxID'] = matched.loc[mask].map(manual_twist_virus)\n",
    "\n",
    "cols = [\"prot_id\", \"allergome_name\", \"SDAP_name\"]\n",
    "df[cols] = df[cols].replace(\"False\", pd.NA)\n",
    "df = df.where(~df.isna(), pd.NA)"
   ],
   "id": "42e48b1227c1f7a2",
   "outputs": [],
   "execution_count": 415
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:05:43.181244Z",
     "start_time": "2025-07-05T17:05:42.933529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "manual_twist_abbv_allergen_name_mapping = {'Hom s ':\"9606\", 'Dan re ':\"117571\", 'Mus a ':\"214687\",\n",
    "       'Sola l ':\"4081\", 'Str py ':\"1314\", 'Dro pp ':\"46245\",\n",
    "       'Fel d ':\"9685\", 'Ory la ':\"8090\",\n",
    "       'Can f ':\"9615\", 'Nas vi ':\"7425\", 'Mala s ':\"1230383\", 'Per a ':\"6978\", 'Gal d ':\"9031\",\n",
    "       'Tri a ':\"4565\", 'Mel g ':\"9103\", 'Bra di ':\"15368\", 'Asp aw ':\"1033177\", 'Bomb m ':\"7091\",\n",
    "       'Tak ru ':\"31033\", 'Ore ni ':\"8128\",\n",
    "       'Asp fl ':\"332952\", 'Acy pi ':\"133076\",\n",
    "       'Gly m ':\"3847\", 'Str dy ':\"370554\",\n",
    "       'Equ c ':\"9796\", 'Hor v ':\"4513\",\n",
    "       'Act d ':\"3627\", 'Aed a ':\"7159\", 'Aln g ':\"3517\", 'Alt a ':\"5599\",\n",
    "       'Api m ':\"7460\", 'Ara h ':\"3818\", 'Ara t ':\"3702\", 'Asp f ':\"746128\",\n",
    "       'Asp o ':\"5062\", 'Bet v ':\"3505\",\n",
    "       'Blo t ':\"40697\",\n",
    "       'Bos d ':\"9913\", 'Bra n ':\"3711\",\n",
    "       'Cand a ':\"5476\", 'Cap a ':\"4072\", 'Cas s ':\"21020\", 'Chi t ':\"7155\",\n",
    "       'Cla h ':\"29918\", 'Cor a ':\"13451\", 'Cup a ':\"257620\", 'Cur l ':\"5503\", 'Den n ':\"51109\",\n",
    "       'Der f ':\"6954\", 'Der p ':\"6956\",\n",
    "       'Dic v ':\"29172\", 'Har a ':\"115357\", 'Hev b ':\"3981\",\n",
    "       'Hom a ':\"6706\", 'Jug r ':\"51240\", 'Lol p ':\"4522\", 'Lyc e ':\"357543\",\n",
    "       'Mal d ':\"3750\", 'Ole e ':\"4146\",\n",
    "       'Ory s ':\"4530\", 'Pha v ':\"3885\", 'Phl p ':\"15957\", 'Pol e ':\"27506\",\n",
    "       'Pol m ':\"91422\", 'Pru du ':\"3755\", 'Sin a ':\"3728\", 'Sol g ':\"121131\",\n",
    "       'Sol s ':\"176597\", 'Sola t ':\"4113\", 'Sor h ':\"4560\", 'Tri t ':\"34387\", 'Zea m ':\"4577\"}\n",
    "\n",
    "mask=(~(df[\"allergome_name\"].notna() & (df[\"allergome_name\"] != \"False\") & (df[\"allergome_name\"].str.strip() != \"\")) & (df[\"prot_id\"].isna()) & (df[\"SDAP_name\"].notna() & (df[\"SDAP_name\"] != \"False\") & (df[\"SDAP_name\"].str.strip() != \"\")) & df['TaxID'].isna())\n",
    "\n",
    "for prefix, taxid in manual_twist_abbv_allergen_name_mapping.items():\n",
    "    sel = mask & df[\"SDAP_name\"].str.startswith(prefix, na=False)\n",
    "    df.loc[sel, \"TaxID\"] = taxid"
   ],
   "id": "71cdf5a36d40fdea",
   "outputs": [],
   "execution_count": 416
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T17:06:06.537816Z",
     "start_time": "2025-07-05T17:06:06.266695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_wp_id(text):\n",
    "    \"\"\"Return the first WP_… accession (with optional version) in the text, else None.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    m = re.search(r'(WP_[0-9]+(?:\\.[0-9]+)?)', text)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "# Rows still missing organism, prot_id, and TaxID but have gened_comments\n",
    "mask_need = (\n",
    "    df['Organism_complete_name'].isna()\n",
    "    & df['prot_id'].isna()\n",
    "    & df['TaxID'].isna()\n",
    "    & df['gened_comments'].notna()\n",
    ")\n",
    "# Extract WP_… accession into prot_id\n",
    "df.loc[mask_need, 'prot_id'] = df.loc[mask_need, 'gened_comments'].map(extract_wp_id)\n",
    "\n",
    "# list of peptides that have weird organism name, curate them manually based on my mapping dict\n",
    "to_curate_df = pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/to_curate_twist.csv\", index_col=0, low_memory=False)\n",
    "for virus, taxid in manual_twist_virus.items():\n",
    "    virus = re.escape(virus)\n",
    "    mask = (df.index.isin(to_curate_df.index) & df['Organism_complete_name'].str.contains(virus, case=False, na=False) & df['TaxID'].isna())\n",
    "    df.loc[mask, 'TaxID'] = taxid\n",
    "\n",
    "for bac, taxid in mapping_twist_bac.items():\n",
    "    bac = re.escape(bac)\n",
    "    mask = (df['Organism_complete_name'].str.contains(bac, case=False, na=False) & df['TaxID'].isna())\n",
    "    df.loc[mask, 'TaxID'] = taxid\n",
    "\n",
    "#clean metadata\n",
    "df.to_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/missing_twist_peptide_annotations_cleaned_addTaxIDcol.csv\")"
   ],
   "id": "4d713154d54f5fa8",
   "outputs": [],
   "execution_count": 417
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T19:11:06.030530Z",
     "start_time": "2025-07-05T19:10:57.222513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_full_name(df):\n",
    "    fn = df['full name'].astype(str)\n",
    "\n",
    "    # If “fummy_name … <bold>” exists, extract only the text in between\n",
    "    fn = fn.str.replace(\n",
    "        r'.*?fummy_name\\s*(.*?)\\s*<bold>.*',\n",
    "        r'\\1',\n",
    "        flags=re.IGNORECASE | re.DOTALL,\n",
    "        regex=True\n",
    "    )\n",
    "\n",
    "    # Strip leading “positive…(from uniprot):” prefix\n",
    "    fn = fn.str.replace(\n",
    "        r'^(?:positive|negative)\\s*\\((?:from uniprot|exact)\\):\\s*',\n",
    "        \"\",\n",
    "        flags=re.IGNORECASE,\n",
    "        regex=True\n",
    "    )\n",
    "\n",
    "\n",
    "    # Remove *any* leading accession ending in .<digits> plus space\n",
    "    fn = fn.str.replace(\n",
    "        r'^\\S+\\.\\d+\\s+',\n",
    "        '',\n",
    "        regex=True\n",
    "    )\n",
    "\n",
    "    # Keep only up to the first ampersand (with or without spaces)\n",
    "    fn = fn.str.split(r'\\s*&\\s*', regex=True).str[0]\n",
    "\n",
    "    # Remove numeric parentheticals like “(1/1)”\n",
    "    fn = fn.str.replace(r'\\s*\\(\\d+/\\d+\\)', \"\", regex=True)\n",
    "\n",
    "    # **Only** strip a bracketed suffix at end of string (e.g. `[foo]` at end)\n",
    "    fn = fn.str.replace(r'\\s*\\[[^\\]]+\\]$', '', regex=True)\n",
    "\n",
    "    # Remove any remaining bracketed notes [like this]\n",
    "    #fn = fn.str.replace(r'\\[.*?\\]', \"\", regex=True)\n",
    "   #  Strip any trailing unclosed “[…” at end\n",
    "    fn = fn.str.replace(r'\\s*\\[[^\\]]*$', '', regex=True)\n",
    "    #  Trim whitespace\n",
    "    fn = fn.str.strip()\n",
    "\n",
    "\n",
    "    df['full name'] = fn\n",
    "    return df\n",
    "\n",
    "df_all = pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/twist_library_with_info.csv\", index_col=0, low_memory=False)\n",
    "df_all = clean_full_name(df_all)\n",
    "mask = df_all['full name'].astype(str).str.startswith(\"allergen_name\", na=False)\n",
    "df_all.loc[mask, 'full name'] = pd.NA\n",
    "\n",
    "# 1) Replace the literal \"False\" and empty strings in your source columns with real missing\n",
    "source_cols = [\n",
    "    'allergome_name',\n",
    "    'allergen_name',\n",
    "    'allergenonline_name',\n",
    "    'SDAP_name',\n",
    "    'iedb_name',\n",
    "    'full name',\n",
    "    'phage_name'\n",
    "]\n",
    "\n",
    "df_all[source_cols] = (\n",
    "    df_all[source_cols]\n",
    "      .replace(\"False\", pd.NA)     # turn the string \"False\" into <NA>\n",
    "      .replace(\"FALSE\", pd.NA)     # turn the string \"False\" into <NA>\n",
    "      .replace(r\"^Unassigned.*\", pd.NA, regex=True) # anything starting with Unassigned\n",
    "      .replace(\"\",     pd.NA)     # turn empty strings into <NA>\n",
    ")\n",
    "\n",
    "# 2) Coalesce (take the first non-null) across those columns into Description\n",
    "df_all['Description'] = (\n",
    "    df_all[source_cols]\n",
    "      .bfill(axis=1)              # back-fill across columns\n",
    "      .iloc[:, 0]                 # then take the first column\n",
    ")\n",
    "\n",
    "# 3) Clean off any \" (…)\" and whitespace\n",
    "df_all['Description'] = df_all['Description'].apply(\n",
    "    lambda x: re.sub(r\"\\s*\\(.*\", \"\", x).strip() if isinstance(x, str) else x\n",
    ")"
   ],
   "id": "2b02c127eecd4a31",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1939077/920444864.py:74: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_all[source_cols]\n"
     ]
    }
   ],
   "execution_count": 518
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T19:13:57.442805Z",
     "start_time": "2025-07-05T19:13:56.942673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tmp_all = df_all[[\"aa_seq\",\"pos\",\"len_seq\",\"Description\", 'is_auto', 'is_infect', 'is_EBV', 'is_phage', 'is_allergens', 'is_influenza']]\n",
    "tmp_all.loc[:,\"aa_seq\"] = tmp_all[\"aa_seq\"].str.replace(r\"\\(.*\", \"\", regex=True).str.strip()\n",
    "#tmp_all.rename(columns={\"prot_name\": \"Description\"}, inplace=True)\n",
    "tmp_annot = pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/twist_completeAnnotation_library_with_lineages.csv\", index_col=0, low_memory=False)\n",
    "pd.merge(tmp_all, tmp_annot, how=\"left\", left_index=True, right_index=True).to_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/twist_library_with_lineages_important_info.csv\")"
   ],
   "id": "1cfb1419fdd7ebf8",
   "outputs": [],
   "execution_count": 522
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Concatanate",
   "id": "92fa96e97961078f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T19:43:36.722892Z",
     "start_time": "2025-07-06T19:43:34.320089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agilent = pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/agilent_library_with_lineages_important_info.csv\", index_col=0, low_memory=False)\n",
    "twist = pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/twist_library_with_lineages_important_info.csv\", index_col=0, low_memory=False)\n",
    "corona = pd.read_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/corona2_library_with_lineages_important_info.csv\", index_col=0, low_memory=False)"
   ],
   "id": "6cf42414afd4aed6",
   "outputs": [],
   "execution_count": 523
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T20:32:18.704466Z",
     "start_time": "2025-07-06T20:32:17.347770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# concatenate them\n",
    "combined = pd.concat([agilent, twist, corona], axis=0, join=\"outer\")\n",
    "cols = combined.columns.tolist()\n",
    "cols.remove('is_allergens')\n",
    "cols.remove('is_influenza')\n",
    "cols.remove('is_phage')\n",
    "idx = cols.index('domain')\n",
    "cols.insert(idx, 'is_allergens')\n",
    "combined = combined[cols]\n",
    "combined.to_csv(\"/home/creyna/Vogl-lab_Projects_git/Annotations/combined_libraries_with_lineages_important_info.csv\")"
   ],
   "id": "e2014696851abee2",
   "outputs": [],
   "execution_count": 544
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5869d89fc9d11fe2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
